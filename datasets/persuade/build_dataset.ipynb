{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_essay(text):\n",
    "    # normalize text\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # remove citations\n",
    "    m = re.search(r\"\\n(Work Cited|Works Cited)\", text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        text = text[: m.start()]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_mapping = {\n",
    "    1: \"Phones and driving\",\n",
    "    2: \"Car-free cities\",\n",
    "    3: \"Summer projects\",\n",
    "    4: '\"A Cowboy Who Rode the Waves\"',\n",
    "    5: \"Mandatory extracurricular activities\",\n",
    "    6: \"Exploring Venus\",\n",
    "    7: \"Facial action coding system\",\n",
    "    8: \"The Face on Mars\",\n",
    "    9: \"Community service\",\n",
    "    10: \"Grades for extracurricular activities\",\n",
    "    11: \"Driverless cars\",\n",
    "    12: \"Does the electoral college work?\",\n",
    "    13: \"Cell phones at school\",\n",
    "    14: \"Distance learning\",\n",
    "    15: \"Seeking multiple opinions\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persuade corpus\n",
    "persuade = pd.read_csv(\"persuade_final_cleaned.csv\")\n",
    "persuade = persuade[[\"text\", \"generated\", \"prompt\"]]\n",
    "\n",
    "# DAIGT public train set\n",
    "public_train = pd.read_csv(\"train_essays.csv\")\n",
    "\n",
    "# Feedback Prize OUTFOX dataset\n",
    "feedback_train_human = pkl.load(\n",
    "    open(\"../feedback_OUTFOX/common/train/train_humans.pkl\", \"rb\")\n",
    ")\n",
    "feedback_test_human = pkl.load(\n",
    "    open(\"../feedback_OUTFOX/common/test/test_humans.pkl\", \"rb\")\n",
    ")\n",
    "feedback_valid_human = pkl.load(\n",
    "    open(\"../feedback_OUTFOX/common/valid/valid_humans.pkl\", \"rb\")\n",
    ")\n",
    "feedback_train_chatgpt = pkl.load(\n",
    "    open(\"../feedback_OUTFOX/chatgpt/train/train_lms.pkl\", \"rb\")\n",
    ")\n",
    "feedback_test_chatgpt = pkl.load(\n",
    "    open(\"../feedback_OUTFOX/chatgpt/test/test_lms.pkl\", \"rb\")\n",
    ")\n",
    "feedback_valid_chatgpt = pkl.load(\n",
    "    open(\"../feedback_OUTFOX/chatgpt/valid/valid_lms.pkl\", \"rb\")\n",
    ")\n",
    "feedback_human = pd.DataFrame(\n",
    "    [*feedback_train_human, *feedback_test_human, *feedback_valid_human]\n",
    ")\n",
    "feedback_chatgpt = pd.DataFrame(\n",
    "    [*feedback_train_chatgpt, *feedback_test_chatgpt, *feedback_valid_chatgpt]\n",
    ")\n",
    "feedback = pd.concat([feedback_human, feedback_chatgpt])\n",
    "feedback.rename({0: \"text\"}, axis=1, inplace=True)\n",
    "feedback[\"generated\"] = np.concatenate(\n",
    "    [np.zeros(len(feedback_human)), np.ones(len(feedback_chatgpt))]\n",
    ")\n",
    "feedback[\"text\"] = feedback[\"text\"].apply(clean_essay)\n",
    "feedback.reset_index(drop=True, inplace=True)\n",
    "feedback.to_csv(\"../feedback.csv\", index=False)\n",
    "\n",
    "# Claude Instant dataset\n",
    "claude = pd.read_csv(\"../claude_instant.csv\")\n",
    "claude = pd.DataFrame(\n",
    "    pd.concat(\n",
    "        [\n",
    "            claude[\"essay_text\"],\n",
    "            pd.Series(1, index=claude.index),\n",
    "            claude[\"prompt_id\"].apply(lambda x: prompt_mapping[x]),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "claude.rename(\n",
    "    {\"essay_text\": \"text\", 1: \"generated\", \"prompt_id\": \"prompt\"}, axis=1, inplace=True\n",
    ")\n",
    "claude[\"text\"] = claude[\"text\"].apply(clean_essay)\n",
    "\n",
    "# Llama 70B and Falcon 180B dataset\n",
    "llama_falcon = pd.read_csv(\"../llama_falcon/llama_falcon_v3.csv\")\n",
    "llama_falcon = llama_falcon[[\"text\", \"generated\", \"prompt_name\"]]\n",
    "llama_falcon.rename({\"prompt_name\": \"prompt\"}, axis=1, inplace=True)\n",
    "llama_falcon[\"text\"] = llama_falcon[\"text\"].apply(clean_essay)\n",
    "\n",
    "# Llama 13B\n",
    "llama_a = pd.read_csv(\"../llama_13b/essays_a.csv\")\n",
    "llama_b = pd.read_csv(\"../llama_13b/essays_b.csv\")\n",
    "llama_13b = pd.concat([llama_a, llama_b])[[\"text\", \"generated\", \"prompt_name\"]]\n",
    "llama_13b.rename({\"prompt_name\": \"prompt\"}, axis=1, inplace=True)\n",
    "llama_13b[\"prompt\"] = llama_13b[\"prompt\"].str.replace(\n",
    "    \"A Cowboy Who Rode the Waves\", '\"A Cowboy Who Rode the Waves\"'\n",
    ")\n",
    "llama_13b[\"text\"] = llama_13b[\"text\"].apply(clean_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35144, 4)\n",
      "\n",
      " prompt\n",
      "Facial action coding system              3317\n",
      "Does the electoral college work?         3203\n",
      "Car-free cities                          3093\n",
      "Driverless cars                          3035\n",
      "Exploring Venus                          2995\n",
      "The Face on Mars                         2726\n",
      "\"A Cowboy Who Rode the Waves\"            2495\n",
      "Distance learning                        2308\n",
      "Summer projects                          1890\n",
      "Mandatory extracurricular activities     1832\n",
      "Cell phones at school                    1793\n",
      "Grades for extracurricular activities    1767\n",
      "Seeking multiple opinions                1699\n",
      "Community service                        1669\n",
      "Phones and driving                       1322\n",
      "Name: count, dtype: int64\n",
      "\n",
      " generated\n",
      "0.0    25644\n",
      "1.0     8500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets\n",
    "combined_df = pd.concat([persuade, claude, llama_falcon, llama_13b])\n",
    "print(combined_df.shape)\n",
    "print(\"\\n\", combined_df[\"prompt\"].value_counts())\n",
    "print(\"\\n\", combined_df[\"generated\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"persuade_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
